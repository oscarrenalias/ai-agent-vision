import logging
import os

from huggingface_hub import login
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from langchain_openai import ChatOpenAI

logger = logging.getLogger(__name__)


def Model(id: str):
    # generated by Copilot, should be cleaned up to remove the if-else if we have more models
    if id == "huggingface":
        return HuggingFaceModel()
    elif id == "openai":
        return OpenAIModel()
    else:
        raise Exception(f"Model {id} not found")


class HuggingFaceModel:
    """
    Handles the initialization and interaction with the Hugging Face model.
    """

    repo_id = "Qwen/Qwen2.5-VL-7B-Instruct"
    task = "image-text-to-text"

    model: ChatHuggingFace

    def __init__(self):
        """
        Initialize the Hugging Face model.
        """
        self.initialize_model()

    def initialize_model(self):
        """
        Log into Hugging Face and configure the model.
        """
        login(token=os.getenv("HUGGINGFACEHUB_API_TOKEN"))
        llm = HuggingFaceEndpoint(
            repo_id=self.repo_id,
            task=self.task,
            do_sample=False,
            repetition_penalty=0.0,
        )
        self.model = ChatHuggingFace(llm=llm, verbose=True)

    def get_model(self):
        return self.model


class OpenAIModel:
    """
    Handles the initialization and interaction with the OpenAI model.

    GPT-4o seems to be much more accurate than the model used in the HuggingFace class
    """

    model: ChatOpenAI

    openai_model = "gpt-4o"

    use_cache: bool = True

    def __init__(self, openai_model: str = "gpt-4o", use_cache: bool = True):
        self.openai_model = openai_model
        self.use_cache = use_cache
        self.initialize_model()

    def initialize_model(self):
        # set temperature to 1 if the model has "mini" in its name since the parameter is not supported
        temperature = 1.0 if "mini" in self.openai_model else 0.0

        self.model = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            model=self.openai_model,
            temperature=temperature,
            verbose=True,
        )

        if self.use_cache:
            logger.info(
                "Using LLM cache to speed things up. Keep this in mind if results do not change when altering prompts!"
            )
            set_llm_cache(SQLiteCache(database_path=".langchain.db"))

    def get_model(self):
        return self.model
